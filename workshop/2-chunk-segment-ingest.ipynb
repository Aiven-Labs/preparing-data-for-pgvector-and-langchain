{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing your data for the model\n",
    "\n",
    "Now that you have your data, you need to prepare it for use with our chosen model.\n",
    "\n",
    "We're using the [all-mpnet-base-v2 from Huggingface](https://huggingface.co/sentence-transformers/all-mpnet-base-v2). This is a very common model used for natural language processing and similarity search.\n",
    "\n",
    "To make our content usable with this model, we need to segment our code into chunks.\n",
    "\n",
    "Models will often have a character or a token limit. `allmpnet-base-v2` has a limit of 384 characters, truncating any characters more than that.\n",
    "\n",
    "We want to make sure that we get AS COMPLETE of a thought as possible. That is to say a complete thought split into two segments will likely both be detected vs having a truncated thought that may include irrelevant content.\n",
    "\n",
    "We're going to split our content into tweet-like segments of approximately 300 characters with a buffer around 20 characters. We'll also split on phrase boundaries like punctuation or newlines.\n",
    "\n",
    "**This is a choice**. You can experiment with the parameters to fit your needs. You can also look at other models that have "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to read the contents of our text file.\n",
    "\n",
    "LangChain is a great way to wrap around the work that we're doing in this.\n",
    "\n",
    "LangChain gives us the ability to select our [embeddings](https://python.langchain.com/docs/integrations/text_embedding/). It also gives us an interface to perform a [recursive split by character](https://python.langchain.com/docs/modules/data_connection/document_transformers/recursive_text_splitter/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Make it Happen\n",
    "\n",
    "Let's create a function that:\n",
    "- reads the file\n",
    "- parses the metadata from the file\n",
    "- splits the content into separate documents with unique identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pathlib\n",
    "import os\n",
    "\n",
    "import json\n",
    "import arrow\n",
    "import frontmatter\n",
    "import psycopg\n",
    "import pprint\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from psycopg.rows import dict_row\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Connection parameters\n",
    "_CONNECTION_STRING = os.getenv(\"AIVEN_POSTGRES_SERVICE_URI\")\n",
    "conn = psycopg.connect(_CONNECTION_STRING)\n",
    "\n",
    "fmt = r\"MMMM[\\s+]D[\\w+,\\s+]YYYY\"\n",
    "\n",
    "# define splitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=20,\n",
    "    separators=[\".\", \"!\", \"?\", \"\\n\"],\n",
    "    keep_separator=\"end\",\n",
    ")\n",
    "\n",
    "# define embeddings. These options are all the defaults and not explicitly needed.\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "    model_kwargs={\"device\": \"cpu\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": False},\n",
    ")\n",
    "\n",
    "\n",
    "def load_data(file: pathlib.Path):\n",
    "    \"\"\"Chunk data, create embeddings, and index in Postgres.\"\"\"\n",
    "    frontmatter_post = frontmatter.loads(\n",
    "        file.read_text()\n",
    "    )  # loads the metadata from the file\n",
    "    base_data = {\n",
    "        \"title\": frontmatter_post[\"title\"],\n",
    "        \"show\": \"Conduit\",\n",
    "        \"network\": \"Relay\",\n",
    "        \"network_url\": \"https://relay.fm\",\n",
    "        \"description\": frontmatter_post[\"description\"],\n",
    "        \"url\": frontmatter_post[\"url\"],\n",
    "        \"pub_date\": arrow.get(frontmatter_post[\"pub_date\"], fmt).date().isoformat(),\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "\n",
    "            transcription_sql = (\n",
    "                \"INSERT INTO transcriptions2 (title, content, meta) VALUES (%s, %s, %s)\"\n",
    "            )\n",
    "            cur.execute(\n",
    "                transcription_sql,\n",
    "                (\n",
    "                    frontmatter_post[\"title\"],\n",
    "                    frontmatter_post.content,\n",
    "                    json.dumps(base_data),\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            insert_query = \"\"\"\n",
    "                    INSERT INTO quotes2 (content, embedding, transcription_title)\n",
    "                    VALUES (%s, %s, %s)\n",
    "            \"\"\"\n",
    "\n",
    "            post_chunks = splitter.create_documents([frontmatter_post.content])\n",
    "            #\n",
    "            # Batch embedding generation\n",
    "            chunk_contents = [chunk.page_content for chunk in post_chunks]\n",
    "            chunk_embeddings = embeddings.embed_documents(chunk_contents)\n",
    "\n",
    "            # Prepare batch insert data\n",
    "            docs = [\n",
    "                (\n",
    "                    post_chunk.page_content,\n",
    "                    embedding,\n",
    "                    frontmatter_post[\"title\"],\n",
    "                )\n",
    "                for post_chunk, embedding in zip(post_chunks, chunk_embeddings)\n",
    "            ]\n",
    "\n",
    "            print(f\"{file.name} - {len(docs)} chunks\")\n",
    "            cur.executemany(insert_query, docs)\n",
    "            conn.commit()\n",
    "\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        logging.error(\"Error proccessing %s: %s\" % (file.name, e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our function, we'll pass that function into our opensearch bulk function. This will allow us to ingest the documents one at a time, making it easier to restart in the event of an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = pathlib.Path(\"../transcripts\")\n",
    "\n",
    "for file in directory.iterdir():\n",
    "    load_data(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view the transcription item and a few of the quotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with conn.cursor(row_factory=dict_row) as cur:\n",
    "    cur.execute(\"SELECT * from transcriptions;\")\n",
    "    result = cur.fetchone()\n",
    "    pprint.pprint(f\"result:{result['title']}, {result['meta']}\") \n",
    "    cur.execute(\"SELECT * from quotes LIMIT 5;\")\n",
    "    pprint.pprint(cur.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXTRA** - If you want run this on all of the files. Change the `directory` path in the block above to `../transcripts_complete` and run the block above again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we did a lot! We chunked. We Generated Embeddings. We also added our example document to our OpenSearch index.\n",
    "\n",
    "In the next Notebook, we'll look at what we can do now that our transcripts have embeddings and we can interact with our data in OpenSearch and implement it in our RAG pattern using LangChain.\n",
    "\n",
    "[![Implement our RAG Patter](https://img.shields.io/badge/3-Implement%20in%20RAG-153a5a?style=for-the-badge&labelColor=ec6147)](3-implement-in-rag.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
