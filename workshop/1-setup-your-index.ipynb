{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create your Index for Similarity Search\n",
    "\n",
    "![Converting our Plain Text Docs into chunked docs in an opensearch index](../img/txt-doc-to-pg-docs.png)\n",
    "\n",
    "In order to ingest our transcriptions we need to prepare an opensearch index to store our data.\n",
    "\n",
    "In this workshop, we're ingesting ONLY [our transcription example](../transcripts/transcription_example.txt) but our opensearch index will have hundreds of documents and our final RAG Application will have tens of thousands of documents.\n",
    "\n",
    "---\n",
    "\n",
    "ðŸ” Let's examine the metadata of our document\n",
    "\n",
    "```yaml\n",
    "description: \"Do you have a grip on productivity? Are you worried that external factors could disrupt what youâ€™re doing at any second? Time to put things in a VICE!\"\n",
    "pub_date: \"March 10th, 2022\"\n",
    "title: \"18: Putting External Factors in a VICE Grip \\U0001F5DC\"\n",
    "url: https://relay.fm/conduit/18\n",
    "```\n",
    "\n",
    "This information along with our `content` needs to be mapped out into an index.\n",
    "\n",
    "While all of the metadata is a string we want to setup our metadata to fit our needs which means `pub_date` should be a `date` value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start out by importing our environment variables and loading our imports. Then we'll establish our connection with our OpenSearchÂ®ï¸ service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import psycopg\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Connection parameters\n",
    "_CONNECTION_STRING = os.getenv(\"AIVEN_POSTGRES_SERVICE_URI\")\n",
    "conn = psycopg.connect(_CONNECTION_STRING)\n",
    "print(conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to enable the PGVector Extension and define our tables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with conn.cursor() as cur:\n",
    "    # Check if vector extension exists\n",
    "    get_vector = \"SELECT EXISTS(SELECT 1 FROM pg_extension WHERE extname = 'vector');\"\n",
    "    cur.execute(get_vector)\n",
    "    extension_exists = cur.fetchone()[0]\n",
    "\n",
    "    if not extension_exists:\n",
    "        cur.execute(\"CREATE EXTENSION vector;\")\n",
    "        cur.execute(get_vector)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a function that will create our table and the fields for it.\n",
    "\n",
    "We'll use two tables â€“ one for transcriptions and the other for quotes and their embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table(cursor, table: str, fields_str: str) -> None:\n",
    "    cursor.execute(\n",
    "        f\"SELECT EXISTS(SELECT 1 FROM information_schema.tables WHERE table_name = '{table}');\"\n",
    "    )\n",
    "    table_exists = cursor.fetchone()[0]\n",
    "\n",
    "    if not table_exists:\n",
    "        cursor.execute(f\"CREATE TABLE {table} ({fields_str});\")\n",
    "        logging.info(\"%s table created successfully.\" % table)\n",
    "    else:\n",
    "        logging.debug(\"%s table already exists.\" % table)\n",
    "\n",
    "# Run the function for each table\n",
    "\n",
    "try:\n",
    "    # Establish the connection\n",
    "    with conn.cursor() as cur:\n",
    "        # Check if vector extension exists\n",
    "        transcription_fields = (\n",
    "                \"title TEXT PRIMARY KEY\",\n",
    "                \"content TEXT\",\n",
    "                \"meta JSONB\",\n",
    "        )\n",
    "        \n",
    "        create_table(cur, \"transcriptions\", \",\".join(transcription_fields))\n",
    "\n",
    "        quote_chunks_fields = (\n",
    "            \"id SERIAL PRIMARY KEY\",\n",
    "            \"content TEXT\",\n",
    "            \"embedding vector(768)\",\n",
    "            \"transcription_title TEXT REFERENCES transcriptions(title)\",  # Foreign key to transcriptions table\n",
    "        )\n",
    "\n",
    "        create_table(cur, \"quotes\", \",\".join(quote_chunks_fields))\n",
    "        conn.commit()\n",
    "\n",
    "except Exception as exc:\n",
    "    print(f\"{exc.__class__.__name__}: {exc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with conn.cursor() as cur:\n",
    "    cur.execute(\"SELECT table_name FROM information_schema.tables WHERE table_schema='public' AND table_type='BASE TABLE'\")\n",
    "    tables = cur.fetchall()\n",
    "    print(tables)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we created our PostgreSQL Tables. We looked at the metadata and made sure that the values matched.\n",
    "\n",
    "In the next notebook we'll split our documents to fit our vectorization model and generate embeddings.\n",
    "\n",
    "Move onto the [next notebook](2-chunk-segment-ingest.ipynb) or push the button below\n",
    "\n",
    "[![Chunk and Ingest your Data](https://img.shields.io/badge/2-Chunk%20and%20Ingest%20Your%20Docs-153a5a?style=for-the-badge&labelColor=ec6147)](2-chunk-segment-ingest.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
