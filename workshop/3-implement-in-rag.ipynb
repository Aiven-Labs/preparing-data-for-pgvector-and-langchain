{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing our RAG Application\n",
    "\n",
    "We've walked through the process of ingesting our data. Now we want to setup our search application.\n",
    "\n",
    "There are multiple ways of implementing search that we won't be addressing in this workshop.\n",
    "\n",
    "We're going to implement a similarity search. We can choose to use the OpenSearch®️ SDK or we can use LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Search with Langchain\n",
    "\n",
    "LangChain allows us to connect our existing configurations across our platform to ensure that we settings are consistent. This is paramount to the success of your similarity search in that if you're search parameters differ from the values you selected to load data from, you can run into some issues.\n",
    "\n",
    "LangChain gives us the ability to access a [preexisting OpenSearch instance](https://python.langchain.com/docs/integrations/vectorstores/opensearch/#using-a-preexisting-opensearch-instance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "import psycopg\n",
    "from psycopg.rows import dict_row\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "query = \"how do I create healthy boundaries\"\n",
    "query_embed = f\"[{\", \".join(str(x) for x in embeddings.embed_query(query))}]\"\n",
    "print(query_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take that embedding and perform a vector search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Connection parameters\n",
    "_CONNECTION_STRING = os.getenv(\"AIVEN_POSTGRES_SERVICE_URI\")\n",
    "conn = psycopg.connect(_CONNECTION_STRING)\n",
    "\n",
    "k_results = 4\n",
    "query = \"Tips for checking email\"\n",
    "query_embed = f\"[{\", \".join(str(x) for x in embeddings.embed_query(query))}]\"\n",
    "with conn.cursor(row_factory=dict_row) as cur:\n",
    "    cur.execute(\n",
    "        \"SELECT * FROM quotes ORDER BY embedding <-> %s LIMIT %s;\",\n",
    "        (\n",
    "            query_embed,\n",
    "            k_results,\n",
    "        ),\n",
    "    )\n",
    "    rows = cur.fetchall()\n",
    "    episodes = set()\n",
    "\n",
    "    for row in rows:\n",
    "        episodes.add(\" - \".join((row[\"transcription_title\"], row[\"content\"])))\n",
    "\n",
    "print(f'Here are some episodes that might help you with \"{query}\":')\n",
    "print(\"\\n\".join(episodes))\n",
    "print(\"-------------------\")\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.2\")\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        Offer supportive advice for the question {query} with supporting quotes from \n",
    "        \"{docs}\".\n",
    "\n",
    "        If there are no documents to quote, say \"I don't have any information on that.\"\n",
    "\n",
    "        Mention the quote you're pulling from                                                                     \n",
    "        Don't include quotes from other sources\n",
    "        make responses about 600 characters\n",
    "\"\"\")\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "topic = {\"query\": query, \"docs\": \"\\n\".join([result.page_content for result in results])}\n",
    "for chunks in chain.stream(topic):\n",
    "    print(chunks, end=\"\", flush=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Connection parameters\n",
    "_CONNECTION_STRING = os.getenv(\"AIVEN_POSTGRES_SERVICE_URI\")\n",
    "conn = psycopg.connect(_CONNECTION_STRING)\n",
    "\n",
    "query = \"Tips for Checking Email\"\n",
    "query_embed = f\"[{\", \".join(str(x) for x in embeddings.embed_query(query))}]\"\n",
    "k_results = 4\n",
    "\n",
    "with conn.cursor(row_factory=dict_row) as cur:\n",
    "    cur.execute(\n",
    "        \"SELECT * FROM quotes ORDER BY embedding <-> %s LIMIT %s;\",\n",
    "        (\n",
    "            query_embed,\n",
    "            k_results,\n",
    "        ),\n",
    "    )\n",
    "    rows = cur.fetchall()\n",
    "    episodes = set()\n",
    "\n",
    "    for row in rows:\n",
    "        episodes.add(\" - \".join((row[\"transcription_title\"], row[\"content\"])))\n",
    "\n",
    "print(f'Here are some episodes that might help you with \"{query}\":')\n",
    "print(\"\\n\".join(episodes))\n",
    "print(\"-------------------\")\n",
    "\n",
    "llm = ChatOllama(model=\"deepseek-r1:8b\")\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        In the style of Dr. Suess, make a catchy acronym from these quotes:\n",
    "        \"{docs}\"\n",
    "        \n",
    "\"\"\")\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "topic = {\"query\": query, \"docs\": \"\\n\".join([row[\"content\"] for row in rows])}\n",
    "for chunks in chain.stream(topic):\n",
    "    print(chunks, end=\"\", flush=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is good but we probably don't want to use Ollama in our production environment. Let look at how easy it is to use a different model.\n",
    "\n",
    "> NOTE: ⚠️ The next block uses the OpenAI API which cannot be used without an API key which you will need to pay for.\n",
    "\n",
    "To run the OpenAI example, you need to add your OpenAI API key to the `.env` file.\n",
    "To do this, replace `<REPLACE_WITH_YOUR_OPENAI_API_KEY>` in the block below and run it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!echo \"OPENAI_API_KEY=<REPLACE_WITH_YOUR_OPENAI_API_KEY>\" > .env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now you can run the block below, to make the same sort of query as before, but this time using OpenAI instead of Ollama:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "print(\"-------------------\")\n",
    "print(f'Here are some episodes that might help you with \"{query}\":')\n",
    "episodes = set()\n",
    "for result in results:\n",
    "    episodes.add(f\"{result.metadata[\"title\"]} - {result.metadata[\"url\"]}\")\n",
    "print(\"\\n\".join(episodes))\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"\"\"Offer supportive advice for the question {query} with supporting quotes from \n",
    "     ---\n",
    "     {docs}\n",
    "     ---\n",
    "    Wrap quotes in quotation marks. Don't include quotes from other sources.\n",
    "    If there are no documents to quote, say \"I don't have any information on that.\"\n",
    "    \n",
    "    Mention the quote you're pulling from                                                                     \n",
    "    Don't include quotes from other sources\n",
    "    limit responses to under 1000 characters but use multiple paragraphs for readibility\n",
    "    \"\"\")\n",
    "    ,\n",
    "    (\"user\",\n",
    "     \"{query}\"),\n",
    "])\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "topic = {\"query\": query, \"docs\": \"\\n\".join([result.page_content for result in results])}\n",
    "for chunks in chain.stream(topic):\n",
    "    print(chunks, end=\"\", flush=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
